{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5abd52-c877-4d1a-92f6-5c4e2f8f9c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 19:31:14,302 - INFO - Scraping data for year 2025\n",
      "/var/folders/06/fr02k8p12_z2m6sdrpjqt7zr0000gn/T/ipykernel_96651/2675375573.py:106: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  year_header = soup.find(['h2', 'h3'], text=year)\n",
      "/var/folders/06/fr02k8p12_z2m6sdrpjqt7zr0000gn/T/ipykernel_96651/2675375573.py:110: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  year_header = soup.find('div', text=year)\n",
      "/var/folders/06/fr02k8p12_z2m6sdrpjqt7zr0000gn/T/ipykernel_96651/2675375573.py:114: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for element in soup.find_all(text=True):\n",
      "2025-02-27 19:31:14,306 - INFO - Found 1 records for 2025\n",
      "2025-02-27 19:31:14,307 - INFO - Scraping data for year 2024\n",
      "2025-02-27 19:31:14,310 - INFO - Found 4 records for 2024\n",
      "2025-02-27 19:31:14,311 - INFO - Scraping data for year 2023\n",
      "2025-02-27 19:31:14,313 - INFO - Found 10 records for 2023\n",
      "2025-02-27 19:31:14,314 - INFO - Scraping data for year 2022\n",
      "2025-02-27 19:31:14,316 - INFO - Found 6 records for 2022\n",
      "2025-02-27 19:31:14,317 - INFO - Scraping data for year 2021\n",
      "2025-02-27 19:31:14,319 - INFO - Found 6 records for 2021\n",
      "2025-02-27 19:31:14,319 - INFO - Scraping data for year 2020\n",
      "2025-02-27 19:31:14,321 - INFO - Found 5 records for 2020\n",
      "2025-02-27 19:31:14,321 - INFO - Scraping data for year 2019\n",
      "2025-02-27 19:31:14,323 - INFO - Found 7 records for 2019\n",
      "2025-02-27 19:31:14,324 - INFO - Scraping data for year 2018\n",
      "2025-02-27 19:31:14,325 - INFO - Found 5 records for 2018\n",
      "2025-02-27 19:31:14,326 - INFO - Scraping data for year 2017\n",
      "2025-02-27 19:31:14,328 - INFO - Found 6 records for 2017\n",
      "2025-02-27 19:31:14,328 - INFO - Scraping data for year 2016\n",
      "2025-02-27 19:31:14,329 - INFO - Found 8 records for 2016\n",
      "2025-02-27 19:31:14,330 - INFO - Scraping data for year 2015\n",
      "2025-02-27 19:31:14,332 - INFO - Found 8 records for 2015\n",
      "2025-02-27 19:31:14,332 - INFO - Scraping data for year 2014\n",
      "2025-02-27 19:31:14,334 - INFO - Found 9 records for 2014\n",
      "2025-02-27 19:31:14,334 - INFO - Scraping data for year 2013\n",
      "2025-02-27 19:31:14,336 - INFO - Found 11 records for 2013\n",
      "2025-02-27 19:31:14,336 - INFO - Scraping data for year 2012\n",
      "2025-02-27 19:31:14,338 - INFO - Found 13 records for 2012\n",
      "2025-02-27 19:31:14,338 - INFO - Scraping data for year 2011\n",
      "2025-02-27 19:31:14,340 - INFO - Found 13 records for 2011\n",
      "2025-02-27 19:31:14,340 - INFO - Scraping data for year 2010\n",
      "2025-02-27 19:31:14,342 - INFO - Found 13 records for 2010\n",
      "2025-02-27 19:31:14,342 - INFO - Scraping data for year 2009\n",
      "2025-02-27 19:31:14,344 - INFO - Found 13 records for 2009\n",
      "2025-02-27 19:31:14,344 - INFO - Scraping data for year 2008\n",
      "2025-02-27 19:31:14,346 - INFO - Found 13 records for 2008\n",
      "2025-02-27 19:31:14,346 - INFO - Scraping data for year 2007\n",
      "2025-02-27 19:31:14,348 - INFO - Found 12 records for 2007\n",
      "2025-02-27 19:31:14,348 - INFO - Scraping data for year 2006\n",
      "2025-02-27 19:31:14,350 - INFO - Found 15 records for 2006\n",
      "2025-02-27 19:31:14,351 - INFO - Scraping data for year 2005\n",
      "2025-02-27 19:31:14,352 - INFO - Found 9 records for 2005\n",
      "2025-02-27 19:31:14,353 - INFO - Scraping data for year 2004\n",
      "2025-02-27 19:31:14,355 - INFO - Found 10 records for 2004\n",
      "2025-02-27 19:31:14,355 - INFO - Scraping data for year 2003\n",
      "2025-02-27 19:31:14,357 - INFO - Found 12 records for 2003\n",
      "2025-02-27 19:31:14,357 - INFO - Scraping data for year 2002\n",
      "2025-02-27 19:31:14,358 - INFO - Found 9 records for 2002\n",
      "2025-02-27 19:31:14,362 - INFO - Successfully scraped 218 jackpot records\n",
      "2025-02-27 19:31:14,364 - INFO - Data saved to 'scraped_lottery_jackpot_history.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data:\n",
      "                 Date         Jackpot           Cash Value  \\\n",
      "0    January 17, 2025    $112 million  $49.95 million cash   \n",
      "1   December 27, 2024  $1.269 billion  $571.9 million cash   \n",
      "2  September 10, 2024    $810 million  $409.3 million cash   \n",
      "3        June 4, 2024    $552 million  $260.2 million cash   \n",
      "4      March 26, 2024  $1.128 billion  $536.6 million cash   \n",
      "\n",
      "                                               Place  \\\n",
      "0                                            Arizona   \n",
      "1                                         California   \n",
      "2                                  Sugar Land, Texas   \n",
      "3  Illinois with a ticket purchased online throug...   \n",
      "4                                         New Jersey   \n",
      "\n",
      "                                Ticket Info  \n",
      "0                                     Tempe  \n",
      "1       Sunshine Food and Gas in Cottonwood  \n",
      "2                      the Sol Living Trust  \n",
      "3                          Anonymous winner  \n",
      "4  ShopRite Liquor #781 in Neptune Township  \n",
      "\n",
      "Data Completeness:\n",
      "Date: 218/218 entries filled (100.0%)\n",
      "Jackpot: 218/218 entries filled (100.0%)\n",
      "Cash Value: 39/218 entries filled (17.9%)\n",
      "Place: 17/218 entries filled (7.8%)\n",
      "Ticket Info: 35/218 entries filled (16.1%)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_jackpot_data(text):\n",
    "    \"\"\"\n",
    "    Extract structured jackpot data from text using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text containing jackpot information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Structured jackpot data\n",
    "    \"\"\"\n",
    "    record = {\n",
    "        'Date': None,\n",
    "        'Jackpot': None,\n",
    "        'Cash Value': None,\n",
    "        'Place': None,\n",
    "        'Ticket Info': None\n",
    "    }\n",
    "    \n",
    "    # Extract date\n",
    "    date_match = re.search(r'\\b((?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4})\\b', text)\n",
    "    if date_match:\n",
    "        record['Date'] = date_match.group(1)\n",
    "    \n",
    "    # Extract jackpot amount\n",
    "    jackpot_match = re.search(r'\\$([\\d.,]+ (?:million|billion))', text)\n",
    "    if jackpot_match:\n",
    "        record['Jackpot'] = jackpot_match.group(0)\n",
    "    \n",
    "    # Extract cash value (in parentheses)\n",
    "    cash_match = re.search(r'\\(\\$([\\d.,]+ (?:million|billion) cash)\\)', text)\n",
    "    if cash_match:\n",
    "        record['Cash Value'] = f\"${cash_match.group(1)}\"\n",
    "    \n",
    "    # Extract place information\n",
    "    place_patterns = [\n",
    "        r'won in ([A-Za-z\\s]+);',\n",
    "        r'won in ([A-Za-z\\s]+),',\n",
    "        r'won in ([A-Za-z\\s]+)\\.',\n",
    "        r'won in ([A-Za-z\\s]+)$'\n",
    "    ]\n",
    "    \n",
    "    for pattern in place_patterns:\n",
    "        place_match = re.search(pattern, text)\n",
    "        if place_match:\n",
    "            record['Place'] = place_match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Handle special cases like trusts or anonymous winners\n",
    "    if \"won by\" in text and not record['Place']:\n",
    "        trust_match = re.search(r'won by (the .+?Trust|an? .+?Trust) of (.+?)\\.', text)\n",
    "        if trust_match:\n",
    "            record['Ticket Info'] = trust_match.group(1)\n",
    "            record['Place'] = trust_match.group(2).strip()\n",
    "        else:\n",
    "            anon_match = re.search(r'won by an anonymous player in ([A-Za-z\\s]+)', text)\n",
    "            if anon_match:\n",
    "                record['Place'] = anon_match.group(1).strip()\n",
    "                record['Ticket Info'] = \"Anonymous winner\"\n",
    "    \n",
    "    # Extract ticket information\n",
    "    if not record['Ticket Info']:\n",
    "        ticket_patterns = [\n",
    "            r'ticket sold in ([^;,.]+)',\n",
    "            r'ticket sold at ([^;,.]+)',\n",
    "            r'ticket purchased at ([^;,.]+)',\n",
    "            r'ticket purchased in ([^;,.]+)',\n",
    "            r'ticket purchased online through ([^;,.]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in ticket_patterns:\n",
    "            ticket_match = re.search(pattern, text)\n",
    "            if ticket_match:\n",
    "                record['Ticket Info'] = ticket_match.group(1).strip()\n",
    "                break\n",
    "    \n",
    "    return record\n",
    "\n",
    "def scrape_lottery_by_year(soup, year):\n",
    "    \"\"\"\n",
    "    Scrape lottery jackpot information for a specific year.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML\n",
    "        year (str): The year to scrape\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing jackpot records\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Try different ways to find the year section based on the HTML structure\n",
    "    # 1. Direct h2/h3 with year\n",
    "    year_header = soup.find(['h2', 'h3'], text=year)\n",
    "    \n",
    "    # 2. If not found, look for year in a div\n",
    "    if not year_header:\n",
    "        year_header = soup.find('div', text=year)\n",
    "    \n",
    "    # 3. As a last resort, find the text node with just the year\n",
    "    if not year_header:\n",
    "        for element in soup.find_all(text=True):\n",
    "            if element.strip() == year:\n",
    "                year_header = element.parent\n",
    "                break\n",
    "    \n",
    "    if year_header:\n",
    "        # Find the list items that follow the year header\n",
    "        # This is a common pattern in HTML for jackpot histories\n",
    "        current = year_header.next_sibling\n",
    "        \n",
    "        # Keep going until we find a list or reach the end\n",
    "        while current and not current.name == 'ul':\n",
    "            current = current.next_sibling\n",
    "        \n",
    "        if current and current.name == 'ul':\n",
    "            # Process each list item\n",
    "            for li in current.find_all('li'):\n",
    "                text = li.text.strip()\n",
    "                \n",
    "                # Skip empty items\n",
    "                if not text:\n",
    "                    continue\n",
    "                \n",
    "                # Extract jackpot data\n",
    "                record = extract_jackpot_data(text)\n",
    "                \n",
    "                # Only add records that have at least date and jackpot\n",
    "                if record['Date'] and record['Jackpot']:\n",
    "                    records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "def scrape_lottery_jackpot_history(url):\n",
    "    \"\"\"\n",
    "    Scrape lottery jackpot history from a webpage.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the jackpot history page\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Cleaned and structured jackpot history data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for 4XX/5XX responses\n",
    "        \n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # List to store all jackpot records\n",
    "        all_records = []\n",
    "        \n",
    "        # Find all year sections (common pattern in jackpot history pages)\n",
    "        # Usually these are in reverse chronological order\n",
    "        potential_years = []\n",
    "        \n",
    "        # Find all headings that might contain years\n",
    "        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):\n",
    "            text = heading.text.strip()\n",
    "            # Look for 4-digit years\n",
    "            if re.match(r'^\\d{4}$', text):\n",
    "                potential_years.append(text)\n",
    "        \n",
    "        # If we found potential years, scrape each year's data\n",
    "        if potential_years:\n",
    "            for year in potential_years:\n",
    "                logger.info(f\"Scraping data for year {year}\")\n",
    "                year_records = scrape_lottery_by_year(soup, year)\n",
    "                all_records.extend(year_records)\n",
    "                logger.info(f\"Found {len(year_records)} records for {year}\")\n",
    "        else:\n",
    "            # Fallback: look for any list items that might contain jackpot info\n",
    "            logger.info(\"No year headers found, trying to find jackpot entries directly\")\n",
    "            for li in soup.find_all('li'):\n",
    "                text = li.text.strip()\n",
    "                \n",
    "                # Skip items that don't contain date patterns\n",
    "                if not re.search(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b', text):\n",
    "                    continue\n",
    "                \n",
    "                # Extract jackpot data\n",
    "                record = extract_jackpot_data(text)\n",
    "                \n",
    "                # Only add records that have at least date and jackpot\n",
    "                if record['Date'] and record['Jackpot']:\n",
    "                    all_records.append(record)\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(all_records)\n",
    "        \n",
    "        # Sort by date (if possible)\n",
    "        try:\n",
    "            df['DateObj'] = pd.to_datetime(df['Date'])\n",
    "            df = df.sort_values('DateObj', ascending=False)\n",
    "            df = df.drop('DateObj', axis=1)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not sort by date: {e}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping data: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # URL of the lottery jackpot history page\n",
    "    url = \"https://www.megamillions.com/jackpot-history\"  # Replace with the actual URL\n",
    "    \n",
    "    try:\n",
    "        # Scrape the data\n",
    "        df = scrape_lottery_jackpot_history(url)\n",
    "        \n",
    "        # Print information about the scraped data\n",
    "        logger.info(f\"Successfully scraped {len(df)} jackpot records\")\n",
    "        print(\"\\nSample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Print statistics on how many entries have values in each column\n",
    "        print(\"\\nData Completeness:\")\n",
    "        for column in df.columns:\n",
    "            filled = df[column].notna().sum()\n",
    "            percentage = filled / len(df) * 100\n",
    "            print(f\"{column}: {filled}/{len(df)} entries filled ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = 'scraped_lottery_jackpot_history.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Data saved to '{output_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef5966-2d44-4e92-9009-10584fc71245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
